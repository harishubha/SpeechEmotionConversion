# train_ddp.py
import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.distributed as dist
import torch.multiprocessing as mp
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler
from torch.utils.data import DataLoader, Dataset
from torch.cuda.amp import autocast, GradScaler
from tqdm import tqdm

import numpy as np
import pandas as pd
import soundfile as sf
import torchaudio
import torchcrepe

# ------------------ CONFIG ------------------
DATA_ROOT = "/data/b22_shruti_chaudhary/.cache/kagglehub/datasets/phantasm34/emovdb-sorted/versions/1"
SAMPLE_RATE = 16000
HOP_LENGTH = 256

# note: do not use a global `device` variable for DDP processes; use per-rank device inside run()
# but keep this for non-DDP parts if needed
# device = "cuda" if torch.cuda.is_available() else "cpu"

# ------------------ METADATA ------------------
discrete = torch.load("discrete_units.pt", map_location="cpu", weights_only=False)
paths_kaggle = discrete["paths"]
units_dedup = discrete["units_dedup"]

def to_rel(p):
    p = p.replace("\\", "/")
    return "/".join(p.split("/")[-3:])

rel_from_kaggle = [to_rel(p) for p in paths_kaggle]

df = pd.read_csv("emovdb_manifest.csv")
df["relpath"] = df["filepath"].apply(to_rel)

label_map = torch.load("label_mappings.pt", map_location="cpu")
emotion2id = label_map["emotion2id"]
id2emotion = {v: k for k, v in emotion2id.items()}
num_emotions = len(emotion2id)

spk_table = torch.load("speaker_table.pt", map_location="cpu")
speaker2id = spk_table["speaker2id"]
id2speaker = {v: k for k, v in speaker2id.items()}
spk_dim = spk_table["spk_dim"]
num_speakers = len(speaker2id)

prosody_data = torch.load("prosody_data.pt", map_location="cpu")
prosody_entries = prosody_data["entries"]

rel2units_dedup = {to_rel(p): u for p, u in zip(paths_kaggle, units_dedup)}

K = 200
PAD = 0
SHIFT = 1
VOCAB_SIZE = K + SHIFT + 2

# ------------------ HELPERS ------------------
def get_local_path(rel):
    return os.path.join(DATA_ROOT, rel)

def get_speaker_id(relpath):
    row = df[df["relpath"] == relpath]
    if row.empty:
        return 0
    spk = row.iloc[0]["speaker"]
    return speaker2id.get(spk, 0)

def get_emotion_id(relpath):
    row = df[df["relpath"] == relpath]
    if row.empty:
        return 0
    emo = row.iloc[0]["emotion"]
    return emotion2id.get(emo, 0)

# (kept for reference; not used inside dataset since we load cached f0)
def extract_f0_crepe(wav, sr, hop_length, fmin=50.0, fmax=500.0):
    if sr != SAMPLE_RATE:
        wav_t = torch.tensor(wav, dtype=torch.float32).unsqueeze(0)
        wav_t = torchaudio.functional.resample(wav_t, sr, SAMPLE_RATE)
        wav = wav_t.squeeze(0).numpy()
        sr = SAMPLE_RATE
    audio_t = torch.tensor(wav, dtype=torch.float32, device="cuda").unsqueeze(0)
    with torch.no_grad():
        f0 = torchcrepe.predict(
            audio_t,
            sr,
            hop_length,
            fmin,
            fmax,
            model="full",
        ).squeeze(0).cpu().numpy()
    return f0

# ------------------ DATASET (uses cached f0) ------------------
class VocoderDataset(Dataset):
    def __init__(self, entries, max_frames=256, f0_cache_dir="f0_cache"):
        self.entries = entries
        self.max_frames = max_frames
        self.f0_cache_dir = f0_cache_dir

    def __len__(self):
        return len(self.entries)

    def load_cached_f0(self, rel, T):
        fname = rel.replace("/", "_") + ".pt"
        f0_path = os.path.join(self.f0_cache_dir, fname)
        if os.path.exists(f0_path):
            f0 = torch.load(f0_path).cpu().numpy()
        else:
            # fallback to zeros if missing
            f0 = np.zeros((1,), dtype=np.float32)
        if len(f0) <= 1:
            return np.zeros(T, dtype=np.float32)
        x_old = np.linspace(0, 1, num=len(f0))
        x_new = np.linspace(0, 1, num=T)
        return np.interp(x_new, x_old, f0).astype(np.float32)

    def __getitem__(self, idx):
        e = self.entries[idx]
        rel = e["relpath"]
        units_d = e["units_dedup"]
        dur = e["dur"]
        spk_id = get_speaker_id(rel)
        emo_id = get_emotion_id(rel)
        local_path = get_local_path(rel)

        wav, sr = sf.read(local_path)
        if wav.ndim > 1:
            wav = wav.mean(axis=1)
        if sr != SAMPLE_RATE:
            wav_t = torch.from_numpy(wav).float().unsqueeze(0)
            wav_t = torchaudio.functional.resample(wav_t, sr, SAMPLE_RATE)
            wav = wav_t.squeeze(0).numpy()
            sr = SAMPLE_RATE

        u = units_d.long()
        d = dur.long()
        unit_frames = []
        for uid, L in zip(u.tolist(), d.tolist()):
            if L <= 0:
                continue
            unit_frames.extend([uid] * L)
        unit_frames = torch.tensor(unit_frames, dtype=torch.long)
        if len(unit_frames) == 0:
            unit_frames = torch.zeros(1, dtype=torch.long)
        T = len(unit_frames)
        max_T = self.max_frames
        if T > max_T:
            start = np.random.randint(0, T - max_T + 1)
            unit_frames = unit_frames[start : start + max_T]
            T = max_T
        else:
            pad_T = max_T - T
            unit_frames = torch.cat(
                [unit_frames, torch.full((pad_T,), PAD, dtype=torch.long)]
            )
            T = max_T

        audio_len = T * HOP_LENGTH
        if len(wav) < audio_len:
            pad = audio_len - len(wav)
            wav = np.pad(wav, (0, pad))
        else:
            start = np.random.randint(0, len(wav) - audio_len + 1)
            wav = wav[start : start + audio_len]

        # load cached F0 (must exist from preprocessing step)
        f0_full = self.load_cached_f0(rel, T)
        f0_frames = torch.tensor(f0_full, dtype=torch.float32)
        wav = torch.tensor(wav, dtype=torch.float32)

        return {
            "units": unit_frames,
            "f0": f0_frames,
            "audio": wav,
            "spk_id": torch.tensor(spk_id, dtype=torch.long),
            "emo_id": torch.tensor(emo_id, dtype=torch.long),
        }

def collate_vocoder(batch):
    units = torch.stack([b["units"] for b in batch])
    f0 = torch.stack([b["f0"] for b in batch])
    audio = torch.stack([b["audio"] for b in batch])
    spk = torch.stack([b["spk_id"] for b in batch])
    emo = torch.stack([b["emo_id"] for b in batch])
    return {
        "units": units,
        "f0": f0,
        "audio": audio,
        "spk_id": spk,
        "emo_id": emo,
    }

# ------------------ MODEL: SpeakerLookup + architecture ------------------
class SpeakerLookup(nn.Module):
    def __init__(self, num_speakers, spk_dim):
        super().__init__()
        self.embedding = nn.Embedding(num_speakers, spk_dim)
    def forward(self, spk_ids):
        return self.embedding(spk_ids)

# instantiate spk_lookup globally and load saved weights
spk_lookup = SpeakerLookup(num_speakers, spk_dim)
if "state_dict" in spk_table:
    try:
        spk_lookup.load_state_dict(spk_table["state_dict"])
    except Exception:
        # fallback: try full table if different naming
        try:
            spk_lookup.load_state_dict(spk_table)
        except Exception:
            pass
spk_lookup.eval()

# model hyperparams
emo_dim = 32
unit_emb_dim = 256
cond_dim = unit_emb_dim + 1 + emo_dim + emo_dim

class VocoderConditioner(nn.Module):
    def __init__(self, vocab_size, num_emotions, spk_dim, unit_emb_dim=256, emo_dim=32):
        super().__init__()
        self.unit_emb = nn.Embedding(vocab_size, unit_emb_dim, padding_idx=PAD)
        self.emo_emb = nn.Embedding(num_emotions, emo_dim)
        self.proj_spk = nn.Linear(spk_dim, emo_dim)
    def forward(self, units, f0, spk_vecs, emo_ids):
        u = self.unit_emb(units + SHIFT)
        f0 = f0.unsqueeze(-1)
        emo = self.emo_emb(emo_ids).unsqueeze(1)
        spk_p = self.proj_spk(spk_vecs).unsqueeze(1)
        emo_exp = emo.expand(-1, u.size(1), -1)
        spk_exp = spk_p.expand(-1, u.size(1), -1)
        cond = torch.cat([u, f0, emo_exp, spk_exp], dim=-1)
        return cond.transpose(1, 2)

class ResBlock(nn.Module):
    def __init__(self, channels, kernel_size=3, dilation=(1, 3, 5)):
        super().__init__()
        self.convs1 = nn.ModuleList([
            nn.utils.weight_norm(
                nn.Conv1d(channels, channels, kernel_size, 1,
                          padding=d * (kernel_size - 1) // 2, dilation=d)
            )
            for d in dilation
        ])
        self.convs2 = nn.ModuleList([
            nn.utils.weight_norm(
                nn.Conv1d(channels, channels, kernel_size, 1,
                          padding=(kernel_size - 1) // 2)
            )
            for _ in dilation
        ])
    def forward(self, x):
        for c1, c2 in zip(self.convs1, self.convs2):
            y = F.leaky_relu(x, 0.1)
            y = c1(y)
            y = F.leaky_relu(y, 0.1)
            y = c2(y)
            x = x + y
        return x

class HiFiGAN_Generator(nn.Module):
    def __init__(self, cond_dim, channels=512, upsample_rates=(8, 8, 2, 2), upsample_kernel_sizes=(16, 16, 4, 4)):
        super().__init__()
        self.conv_pre = nn.utils.weight_norm(nn.Conv1d(cond_dim, channels, 7, 1, padding=3))
        self.ups = nn.ModuleList()
        self.resblocks = nn.ModuleList()
        in_ch = channels
        for u, k in zip(upsample_rates, upsample_kernel_sizes):
            self.ups.append(
                nn.utils.weight_norm(
                    nn.ConvTranspose1d(in_ch, in_ch // 2, k, u, padding=(k - u) // 2)
                )
            )
            in_ch = in_ch // 2
            self.resblocks.append(nn.ModuleList([ResBlock(in_ch) for _ in range(3)]))
        self.conv_post = nn.utils.weight_norm(nn.Conv1d(in_ch, 1, 7, 1, padding=3))
    def forward(self, x):
        x = self.conv_pre(x)
        for up, rbs in zip(self.ups, self.resblocks):
            x = F.leaky_relu(x, 0.1)
            x = up(x)
            out = 0
            for rb in rbs:
                out += rb(x)
            x = out / len(rbs)
        x = F.leaky_relu(x, 0.1)
        x = self.conv_post(x)
        return torch.tanh(x)

class MPD_Block(nn.Module):
    def __init__(self, period):
        super().__init__()
        self.period = period
        self.convs = nn.ModuleList([
            nn.utils.weight_norm(nn.Conv2d(1, 32, (5, 1), (3, 1), padding=(2, 0))),
            nn.utils.weight_norm(nn.Conv2d(32, 128, (5, 1), (3, 1), padding=(2, 0))),
            nn.utils.weight_norm(nn.Conv2d(128, 512, (5, 1), (3, 1), padding=(2, 0))),
            nn.utils.weight_norm(nn.Conv2d(512, 1024, (5, 1), (3, 1), padding=(2, 0))),
            nn.utils.weight_norm(nn.Conv2d(1024, 1024, (5, 1), 1, padding=(2, 0))),
        ])
        self.conv_post = nn.utils.weight_norm(nn.Conv2d(1024, 1, (3, 1), 1, padding=(1, 0)))
    def forward(self, x):
        b, c, t = x.shape
        pad = (self.period - (t % self.period)) % self.period
        x = F.pad(x, (0, pad), mode="reflect")
        t = x.shape[-1]
        x = x.view(b, 1, t // self.period, self.period)
        feats = []
        for l in self.convs:
            x = l(x)
            x = F.leaky_relu(x, 0.1)
            feats.append(x)
        x = self.conv_post(x)
        feats.append(x)
        x = x.reshape(b, -1)
        return x, feats

class MultiPeriodDiscriminator(nn.Module):
    def __init__(self, periods=[2, 3, 5, 7, 11]):
        super().__init__()
        self.blocks = nn.ModuleList([MPD_Block(p) for p in periods])
    def forward(self, x):
        ret = []
        for b in self.blocks:
            score, feat = b(x)
            ret.append((score, feat))
        return ret

class MSD_Block(nn.Module):
    def __init__(self):
        super().__init__()
        self.convs = nn.ModuleList([
            nn.utils.weight_norm(nn.Conv1d(1, 128, 15, 1, padding=7)),
            nn.utils.weight_norm(nn.Conv1d(128, 128, 41, 2, padding=20, groups=4)),
            nn.utils.weight_norm(nn.Conv1d(128, 256, 41, 2, padding=20, groups=16)),
            nn.utils.weight_norm(nn.Conv1d(256, 512, 41, 2, padding=20, groups=16)),
            nn.utils.weight_norm(nn.Conv1d(512, 512, 41, 2, padding=20, groups=16)),
            nn.utils.weight_norm(nn.Conv1d(512, 512, 5, 1, padding=2)),
        ])
        self.conv_post = nn.utils.weight_norm(nn.Conv1d(512, 1, 3, 1, padding=1))
    def forward(self, x):
        feats = []
        for l in self.convs:
            x = l(x)
            x = F.leaky_relu(x, 0.1)
            feats.append(x)
        x = self.conv_post(x)
        feats.append(x)
        return x, feats

class MultiScaleDiscriminator(nn.Module):
    def __init__(self):
        super().__init__()
        self.b0 = MSD_Block()
        self.b1 = MSD_Block()
        self.b2 = MSD_Block()
        self.avg = nn.AvgPool1d(4, 2, padding=2)
    def forward(self, x):
        outs = []
        x1 = x
        outs.append(self.b0(x1))
        x2 = self.avg(x1)
        outs.append(self.b1(x2))
        x3 = self.avg(x2)
        outs.append(self.b2(x3))
        return outs

# ------------------ LOSSES ------------------
def loss_adv_g(scores):
    loss = 0
    for s in scores:
        loss += torch.mean((1 - s) ** 2)
    return loss

def loss_adv_d(real_scores, fake_scores):
    loss = 0
    for r, f in zip(real_scores, fake_scores):
        loss += torch.mean((r - 1) ** 2) + torch.mean(f ** 2)
    return loss

def feature_matching(real_feats, fake_feats):
    loss = 0
    for r, f in zip(real_feats, fake_feats):
        for rr, ff in zip(r, f):
            loss += torch.mean(torch.abs(rr - ff))
    return loss

def log_norm_f0(f0):
    f0 = f0.clone()
    f0[f0 <= 0] = 1.0
    f0 = torch.log(f0)
    m = f0.mean()
    s = f0.std().clamp(min=1e-6)
    return (f0 - m) / s

# ------------------ DDP helpers ------------------
def setup(rank, world_size):
    os.environ['MASTER_ADDR'] = '127.0.0.1'
    os.environ['MASTER_PORT'] = '29500'
    dist.init_process_group("nccl", rank=rank, world_size=world_size)
    torch.cuda.set_device(rank)

def cleanup():
    dist.destroy_process_group()

# ------------------ TRAIN RUN ------------------
def run(rank, world_size):
    setup(rank, world_size)
    device = torch.device(f"cuda:{rank}")

    # dataset + distributed sampler
    ds = VocoderDataset(prosody_entries, max_frames=256, f0_cache_dir="f0_cache")
    sampler = DistributedSampler(ds, num_replicas=world_size, rank=rank, shuffle=True, seed=42)
    loader = DataLoader(
        ds,
        batch_size=4,  # per-process batch size
        sampler=sampler,
        num_workers=4,
        pin_memory=True,
        collate_fn=collate_vocoder,
    )

    # instantiate models and move to device
    generator_net = HiFiGAN_Generator(cond_dim=cond_dim).to(device)
    mpd_net = MultiPeriodDiscriminator().to(device)
    msd_net = MultiScaleDiscriminator().to(device)
    cond_net_local = VocoderConditioner(VOCAB_SIZE, num_emotions, spk_dim, unit_emb_dim=unit_emb_dim, emo_dim=emo_dim).to(device)

    # load checkpoint weights (if exists)
    ckpt_start = "hifigan_vocoder_epoch50.pt"
    if os.path.exists(ckpt_start):
        ck = torch.load(ckpt_start, map_location=device)
        generator_net.load_state_dict(ck["generator"])
        mpd_net.load_state_dict(ck["mpd"])
        msd_net.load_state_dict(ck["msd"])
        cond_net_local.load_state_dict(ck["cond_net"])

    # wrap with DDP
    generator = DDP(generator_net, device_ids=[rank], find_unused_parameters=False)
    mpd = DDP(mpd_net, device_ids=[rank], find_unused_parameters=False)
    msd = DDP(msd_net, device_ids=[rank], find_unused_parameters=False)
    cond_net = DDP(cond_net_local, device_ids=[rank], find_unused_parameters=False)

    # speaker lookup: keep local, move to device
    spk_lookup_local = spk_lookup.to(device)
    spk_lookup_local.eval()

    optim_g = torch.optim.Adam(generator.parameters(), lr=2e-4, betas=(0.8, 0.99))
    optim_d = torch.optim.Adam(list(mpd.parameters()) + list(msd.parameters()), lr=2e-4, betas=(0.8, 0.99))

    scaler = GradScaler()
    num_epochs = 130

    for epoch in range(num_epochs):
        sampler.set_epoch(epoch)
        generator.train()
        mpd.train()
        msd.train()

        total_g = 0.0
        total_d = 0.0

        pbar = tqdm(loader, desc=f"Rank {rank} Epoch {epoch+1}/{num_epochs}", disable=(rank!=0), ncols=120)
        for i, batch in enumerate(pbar):
            units = batch["units"].to(device)
            f0 = batch["f0"].to(device)
            audio = batch["audio"].to(device)
            spk_id = batch["spk_id"].to(device)
            emo_id = batch["emo_id"].to(device)

            # --- DISCRIMINATOR STEP ---
            optim_d.zero_grad()
            with torch.no_grad():
                spk_vecs = spk_lookup_local(spk_id)
                f0n = log_norm_f0(f0)
                cond = cond_net(units, f0n, spk_vecs, emo_id)
                fake_audio = generator(cond).squeeze(1)
                fake_audio = fake_audio[:, :audio.size(1)]

            real = audio.unsqueeze(1)
            fake = fake_audio.unsqueeze(1)

            r_mpd = mpd(real)
            f_mpd = mpd(fake)
            r_msd = msd(real)
            f_msd = msd(fake)

            real_scores = [x[0] for x in r_mpd] + [x[0] for x in r_msd]
            fake_scores = [x[0] for x in f_mpd] + [x[0] for x in f_msd]

            loss_d = loss_adv_d(real_scores, fake_scores)
            scaler.scale(loss_d).backward()
            scaler.step(optim_d)
            scaler.update()

            # --- GENERATOR STEP ---
            optim_g.zero_grad()
            with autocast():
                spk_vecs = spk_lookup_local(spk_id)
                f0n = log_norm_f0(f0)
                cond = cond_net(units, f0n, spk_vecs, emo_id)
                fake_audio = generator(cond).squeeze(1)
                fake_audio = fake_audio[:, :audio.size(1)]
                real = audio.unsqueeze(1)
                fake = fake_audio.unsqueeze(1)

                r_mpd = mpd(real)
                f_mpd = mpd(fake)
                r_msd = msd(real)
                f_msd = msd(fake)

                adv = loss_adv_g([x[0] for x in f_mpd] + [x[0] for x in f_msd])
                fm = feature_matching(
                    [x[1] for x in r_mpd] + [x[1] for x in r_msd],
                    [x[1] for x in f_mpd] + [x[1] for x in f_msd],
                )
                l1 = F.l1_loss(fake_audio, audio)
                loss_g = adv + 10 * l1 + 2 * fm

            scaler.scale(loss_g).backward()
            scaler.step(optim_g)
            scaler.update()

            total_d += loss_d.item()
            total_g += loss_g.item()

            if rank == 0:
                avg_d = total_d / (i + 1)
                avg_g = total_g / (i + 1)
                pbar.set_postfix({
                    "D": f"{loss_d.item():.3f}",
                    "G": f"{loss_g.item():.3f}",
                    "D_avg": f"{avg_d:.3f}",
                    "G_avg": f"{avg_g:.3f}",
                })

        # checkpoint only on rank 0
        if rank == 0 and (epoch + 1) % 10 == 0:
            ckpt = {
                "generator": generator.module.state_dict(),
                "mpd": mpd.module.state_dict(),
                "msd": msd.module.state_dict(),
                "cond_net": cond_net.module.state_dict()
            }
            torch.save(ckpt, f"ddp_hifigan_vocoder_epoch_ft_{epoch+1}.pt")
            print(f"[Rank 0] Saved checkpoint for epoch {epoch+1}")

    cleanup()

def main():
    world_size = torch.cuda.device_count()
    if world_size == 0:
        raise RuntimeError("No CUDA devices found for DDP.")
    mp.spawn(run, args=(world_size,), nprocs=world_size, join=True)

if __name__ == "__main__":
    main()
